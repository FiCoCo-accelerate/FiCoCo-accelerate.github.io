<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta property="og:title"
        content="Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration" />
    <meta property="og:url" content="https://FiCoCo-accelerate.github.io/" />
    <meta property="og:image" content="static/images/head.png" />
    <meta property="og:image:width" content="2048" />
    <meta property="og:image:height" content="1152" />
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>FiCoCo</title>
    <script>
        var x = window.innerWidth;
        function resizeFresh(){
            if(x!=window.innerWidth)
                location.reload();
        }
    </script>
    <link rel="icon" type="image/x-icon" href="static/images/4.png">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/index.js"></script>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head>

<body onresize="resizeFresh()">

    <section class="hero banner">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            <i class="pixart-alpha-icon"></i>
                            <!-- <span style="color: rgb(167, 175, 246);">SimM:</span> -->
                            <span style="color: rgb(165, 203, 255);">Rethinking Token Reduction in MLLMs: </span><br />
                            <!-- A Training-Free Layout Calibration System </br> for Text-to-Image Generation</h1> -->
                            Towards a Unified Paradigm for Training-Free Acceleration</h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=zggQZNAAAAAJ" target="_blank">Yuhang
                                    Han</a><sup>1,â€¡</sup>,</span>
                            <span class="author-block">
                                <a href="https://xuyang-liu16.github.io/" target="_blank">Xuyang
                                    Liu</a><sup>2,â€¡</sup>,</span>
                            <span class="author-block">
                                <a href="https://dingpx.github.io/" target="_blank">Pengxiang
                                    Ding</a><sup>3</sup>,</span>
                            <span class="author-block">
                                <a href="https://milab.westlake.edu.cn/" target="_blank">Donglin
                                    Wang</a><sup>3</sup>,</span>                                    
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=qkpb0CMAAAAJ" target="_blank">
                                    Honggang
                                    Chen</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://qingsenyangit.github.io/" target="_blank">
                                    Qingsen 
                                    Yan</a><sup>1</sup></span>
                            <span class="author-block">
                                <a href="https://kyonhuang.top/" target="_blank">
                                    Siteng 
                                    Huang</a><sup>4,*</sup></span>                  
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Northwestern Polytechnical University</span>&nbsp;&nbsp;<span class="author-block"><sup>2</sup>Sichuan University</span>&nbsp;&nbsp;<span class="author-block"><sup>3</sup>Westlake University</span>&nbsp;&nbsp;<span class="author-block"><sup>4</sup>DAMO Academy, Alibaba Group</span>
                            <span class="eql-cntrb"><small><br><sup>â€¡</sup>Equal contribution. <sup>*</sup>Corresponding author. </small></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2411.17686.pdf" target="_blank"
                                        class="external-link button is-normal is-rounded is-white">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2411.17686" target="_blank"
                                        class="external-link button is-normal is-rounded is-white">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span> 
                                <span class="link-block">
                                    <a href="https://huggingface.co/papers/2411.17686" target="_blank"
                                        class="external-link button is-normal is-rounded is-white">
                                        <span class="icon">ðŸ¤—</span>
                                        <span>Paper page</span>
                                    </a>
                                </span>    
                                <span class="link-block">
                                    <a href="" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                      <i class="fab fa-github"></i>
                                    </span>
                                    <span>Code (Available Soon)</span>
                                  </a>
                                </span>                               
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    
    
    <!-- Image carousel -->
    <!-- <section class="hero is-small">
        <div class="hero-body">
            <section class="section" style="padding-top: 0px;">
                <div class="container is-max-desktop">
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <div class="publication-video-simm">
                                <video width="100%" controls>
                                    <source src="https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/439686710798.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </div>
    </section> -->
    <!-- End image carousel -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            To accelerate the inference of heavy Multimodal Large Language Models (MLLMs), this study rethinks the current landscape of training-free token reduction research.
                            We regret to find that the critical components of existing methods are tightly intertwined, with their interconnections and effects remaining unclear for comparison, transfer, and expansion.
                            Therefore, we propose a unified ''<b>fi</b>lter-<b>co</b>rrelate-<b>co</b>mpress'' paradigm that decomposes the token reduction into three distinct stages within a pipeline, maintaining consistent design objectives and elements while allowing for unique implementations.
                            We additionally demystify the popular works and subsume them into our paradigm to showcase its universality. Finally, we offer a suite of methods grounded in the paradigm, striking a balance between speed and accuracy throughout different phases of the inference.
                            Experimental results across 10 benchmarks indicate that our methods can achieve up to an 82.4% reduction in FLOPs with a minimal impact on performance, simultaneously surpassing state-of-the-art training-free methods.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <!-- <h2 class="title is-3">Samples generated by <i class="pixart-alpha-icon"></i><span style="color: rgb(72, 87, 220);">Ranni</span></h2> -->  <!-- not used before --> 
                <h2 class="title is-3">A Unified Paradigm of Token Reduction</h2>
                <p>
                    We propose a unified ''<b>filter-correlate-compress</b>'' paradigm for training-free token reduction, which includes three stages, each stage responsible for addressing a question:
                </p> 
                <p>
                (1) <b>Filter</b>: ''<i>What token should be discarded?</i>''
                </p>
                <p>
                (2) <b>Correlate</b>: ''<i>Where should discarded information be preserved?</i>''
                </p>
                <p>
                (3) <b>Compress</b>: ''<i>How to fuse the tokens to preserve information?</i>''
                </p>   
                <img loading="lazy" src="static/images/paradigm.png" alt="case" />    
                <p>
                Please refer to our paper to explore how this paradigm decomposes and formulates existing methods.
                </p>       
            </div>
        </div>
    </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <!-- <h2 class="title is-3">Samples generated by <i class="pixart-alpha-icon"></i><span style="color: rgb(72, 87, 220);">Ranni</span></h2> -->  <!-- not used before --> 
                <h2 class="title is-3">Empirical Equivalency of Paradigm</h2>
                <p>
                    The unified paradigm offers several distinct benefits:
                </p> 
                <p>
                (1) <b>Decomposability</b>: The paradigm unfolds the entangled
                token reduction into a structured pipeline with three key
                stages, each with standardized input and output interfaces.
                </p>
                <p>
                (2) <b>Understandability</b>: Each stage within the paradigm is
                characterized by a well-defined design objective and clearly
                specifies the intermediate elements to be implemented.
                </p>
                <p>
                (3) <b>Flexibility</b>: The implementation of the intermediate elements is not restricted,
                allowing the paradigm to accommodate existing methods and facilitate further expansion.
                </p>
                <p></p>
                <p>
                In the following table, we provide empirical evidence to
                illustrate the equivalence between the original methods and
                our deconstructed versions.
                This indicates that our paradigm can
                encompass existing token reduction methods effortlessly.
                </p>
                <img loading="lazy" src="static/images/empirical_equivalency.png" style="max-width: 400px;" alt="case" /> 
            </div>
        </div>
    </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <!-- <h2 class="title is-3">Samples generated by <i class="pixart-alpha-icon"></i><span style="color: rgb(72, 87, 220);">Ranni</span></h2> -->  <!-- not used before --> 
                <h2 class="title is-3">Methodology: FiCoCo</h2>
                <img loading="lazy" src="static/images/FiCoCo.png" alt="case" />   
                <p>
                    Based on the paradigm, we develop a series of methods named <b>FiCoCo</b> that efficiently reduce the amount of visual token without re-training.
                    <b>FiCoCo-V</b> reduces tokens in the visual encoder,
                    <b>FiCoCo-L</b> reduces tokens in the LLM decoder, 
                    and <b>FiCoCo-VL</b> integrates their designs to reduce tokens in both phases, respectively.
                </p>
                <p></p>
                <p>
                    Despite the above figure, we further provide the algorithm illustration for <b>FiCoCo-V</b> and <b>FiCoCo-L</b> to clarify their distinct solutions across three stages.
                </p>            
                <img loading="lazy" src="static/images/algorithm.png" alt="case" />               
            </div>
        </div>
    </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <!-- <h2 class="title is-3">Samples generated by <i class="pixart-alpha-icon"></i><span style="color: rgb(72, 87, 220);">Ranni</span></h2> -->  <!-- not used before --> 
                <h2 class="title is-3">Main Results</h2>
                <p>
                    Performance comparison on TextVQA benchmark:
                </p>
                <img loading="lazy" src="static/images/FLOPs.png" alt="case" />     
                <p>
                Please refer to our paper for detailed experimental results.
                </p>                        
            </div>
        </div>
    </section>
   

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{FiCoCo2024,
    title={Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration}, 
    author={Yuhang Han and Xuyang Liu and Pengxiang Ding and Donglin Wang and Honggang Chen and Qingsen Yan and Siteng Huang},
    year={2024},
    eprint={2411.17686},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}</code></pre>             
        </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This page was built using the <a
                                href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a
                                href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
                            You are free to borrow the of this website, we just ask that you link back to this page in
                            the footer. <br> This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                        <p class="has-text-centered">Total clicks: <span id="busuanzi_value_site_pv"></span></p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>
